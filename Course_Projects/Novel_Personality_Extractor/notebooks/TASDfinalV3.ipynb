{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa5404d",
   "metadata": {},
   "source": [
    "# Novel Characters' Personality Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f7888",
   "metadata": {},
   "source": [
    "This notebook analyzes character personalities from books using NLP techniques.\n",
    "It will extract character mentions, group name variants, and map adjectives to personality traits.\n",
    "Intended use is to provide a list of characters to look for and the book's name, and to receive a list of the traits for each of said characters based on how they're depicted in it. If no list is provided, the code will return traits for all the \"person\" items found through spacy's NER (that also verify a few relevance checks, e.g. number of mentions > 3).\n",
    "\n",
    "All the way through the code, I'll use \"Frankenstein\" for examples to better explain how the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16023811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all the libraries \n",
    "import requests #HTTP requests \n",
    "import spacy #nlp lib\n",
    "import re # finding patterns in text and cleaning\n",
    "from collections import defaultdict, Counter #autoini for dict, counting\n",
    "from typing import Dict, List, Tuple, Set #to document what types dict etc are for people with poor memory\n",
    "import pandas as pd #maybe will be used for some data analysis\n",
    "from dataclasses import dataclass #nicer classes\n",
    "import json #maybe for saving results to JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072c666",
   "metadata": {},
   "source": [
    "Load large spacy model (ENG). Probably the small one can work fine as well. The trasformer-based one could possibly perform better but I'll stick to this as seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc216103",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e93501",
   "metadata": {},
   "source": [
    "The following class will be used for each reference and include where the sentence starts and ends, its content and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a3dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CharacterMention:\n",
    "    \"\"\"Represents a character mention in text\"\"\"\n",
    "    name: str\n",
    "    start: int\n",
    "    end: int\n",
    "    sentence: str #sentence\n",
    "    doc_context: str  #larger sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ea4db",
   "metadata": {},
   "source": [
    "The following class will make up the core part of this work. This design choice came from the intention of trying to make a call for the \"Analyzing\" process as streamlined as possible in order to allow faster testing and utilize, as not having any true loss (or numerical) function to optimize means that any assessment on the performance can only be done by observing the model's behavior on repeated tasks. Due to markdown comments being hard to implement in the middle of a class, I'll use (''') to comment the most important methods of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce48340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CharacterPersonalityAnalyzer:\n",
    "    \"\"\"\n",
    "    This is the main class for analyzing character personality.\n",
    "    The pipeline:\n",
    "    1 Fetch and preprocess books from Project Gutenberg\n",
    "    2 Extract character mentions using NER\n",
    "    3 Group character name variants\n",
    "    4 Extract adjectives associated with characters using dependency parsing\n",
    "    5 Map adjectives to personality traits and calculate scores\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.books = {}  # dict book_name -> text\n",
    "        self.character_mentions = defaultdict(list)  # dict book_name -> list [CharacterMention]\n",
    "        self.character_variants = defaultdict(set)  # dict canonical_name -> {name_variants}\n",
    "        self.personality_traits = self._load_personality_lexicon() #the trait adjective map (dict)\n",
    "        \n",
    "    def _load_personality_lexicon(self):\n",
    "        \"\"\"This acts as a function from adjectives to personality traits.\n",
    "        Inititally it was crafted using Oxford dictionary and my limited knowledge of english, the result was disappointing.\n",
    "        Then I printed the adjectives I was pulling and added some from there, but while proceding it seemed daunting and still failed to generalize,\n",
    "        Finally, defeated by the weakness of us flesh-beings I had a gpt model do it for me, which seems to work better.\n",
    "        Can be greatly expanded upon to improve performance, as several terms in the books are not modern english and are not caught by this mapping.\n",
    "        Output a dictionary mapping trait names to lists of associated adjectives.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'brave': ['brave', 'courageous', 'bold', 'fearless', 'heroic', 'valiant', 'daring', 'gallant', 'intrepid', 'audacious', 'resolute', 'undaunted'],\n",
    "            'kind': ['kind', 'gentle', 'compassionate', 'caring', 'tender', 'benevolent', 'merciful', 'sympathetic', 'altruistic', 'empathetic', 'gracious', 'humane', 'warmhearted'],\n",
    "            'intelligent': ['intelligent', 'smart', 'clever', 'wise', 'brilliant', 'astute', 'shrewd', 'sharp', 'analytical', 'perceptive', 'knowledgeable', 'rational', 'lucid'],\n",
    "            'cruel': ['cruel', 'harsh', 'brutal', 'merciless', 'ruthless', 'savage', 'vicious', 'heartless', 'malevolent', 'sadistic', 'inhumane', 'callous'],\n",
    "            'proud': ['proud', 'arrogant', 'haughty', 'conceited', 'vain', 'pompous', 'boastful', 'overconfident', 'self-important', 'egotistical'],\n",
    "            'humble': ['humble', 'modest', 'meek', 'unassuming', 'self-effacing', 'unpretentious', 'deferential', 'reserved', 'lowly'],\n",
    "            'loyal': ['loyal', 'faithful', 'devoted', 'true', 'steadfast', 'reliable', 'trustworthy', 'constant', 'dedicated', 'dependable'],\n",
    "            'treacherous': ['treacherous', 'deceitful', 'dishonest', 'false', 'traitorous', 'unfaithful', 'duplicitous', 'perfidious', 'scheming', 'two-faced'],\n",
    "            'strong': ['strong', 'powerful', 'mighty', 'robust', 'sturdy', 'stalwart', 'resilient', 'tenacious', 'determined', 'unshakable'],\n",
    "            'weak': ['weak', 'feeble', 'frail', 'fragile', 'delicate', 'vulnerable', 'infirm', 'impotent', 'timid', 'ineffectual'],\n",
    "            'obsessive': ['obsessive', 'fixated', 'compulsive', 'single-minded', 'driven', 'monomaniacal', 'fanatical', 'relentless'],\n",
    "            'remorseful': ['remorseful', 'repentant', 'contrite', 'penitent', 'rueful', 'guilt-ridden', 'ashamed', 'self-reproaching'],\n",
    "            'isolated': ['isolated', 'lonely', 'solitary', 'alienated', 'estranged', 'detached', 'reclusive', 'abandoned'],\n",
    "            'sublime': ['sublime', 'majestic', 'awe-inspiring', 'exalted', 'transcendent', 'solemn', 'grand', 'lofty'],\n",
    "            'sensitive': ['sensitive', 'emotional', 'empathetic', 'tender-hearted', 'perceptive', 'touchy', 'vulnerable', 'expressive'],\n",
    "            'vindictive': ['vindictive', 'vengeful', 'spiteful', 'wrathful', 'punitive', 'resentful', 'malicious', 'retaliatory'],\n",
    "            'naive': ['naive', 'innocent', 'guileless', 'ingenuous', 'unworldly', 'trusting', 'credulous', 'childlike'],\n",
    "            'pious': ['pious', 'devoted', 'reverent', 'spiritual', 'faithful', 'godly', 'righteous', 'churchgoing'],\n",
    "            'passionate': ['passionate', 'fiery', 'ardent', 'fervent', 'zealous', 'impetuous', 'intense', 'emotional'],\n",
    "            'cowardly': ['cowardly', 'timid', 'fearful', 'craven', 'faint-hearted', 'spineless', 'hesitant', 'weak-kneed'],\n",
    "            'heroic': ['heroic', 'righteous', 'noble', 'honorable', 'just', 'virtuous', 'gallant', 'selfless'],\n",
    "            'complex': ['complex', 'conflicted', 'morally ambiguous', 'multifaceted', 'layered', 'ambivalent', 'contradictory'],\n",
    "            'malicious': ['malicious', 'malevolent', 'wicked', 'evil', 'sinister', 'malignant', 'villainous', 'nefarious'],\n",
    "            'resilient': ['resilient', 'determined', 'persistent', 'tenacious', 'steadfast', 'enduring', 'tough', 'unbreakable'],\n",
    "            'worldly-wise': ['worldly-wise', 'savvy', 'seasoned', 'sophisticated', 'experienced', 'astute', 'pragmatic'],\n",
    "            'melancholic': ['melancholic', 'gloomy', 'mournful', 'sorrowful', 'despondent', 'dejected', 'forlorn', 'woeful'],\n",
    "            'manipulative': ['manipulative', 'scheming', 'conniving', 'devious', 'calculating', 'controlling', 'coercive'],\n",
    "            'nurturing': ['nurturing', 'caring', 'motherly', 'protective', 'supportive', 'comforting', 'kind-hearted'],\n",
    "            'just': ['just', 'fair', 'equitable', 'righteous', 'impartial', 'upright', 'principled'],\n",
    "            'charismatic': ['charismatic', 'charming', 'persuasive', 'magnetic', 'captivating', 'engaging', 'likeable'],\n",
    "            'reckless': ['reckless', 'rash', 'impulsive', 'careless', 'foolhardy', 'hotheaded', 'brash'],\n",
    "            'stoic': ['stoic', 'calm', 'composed', 'unflappable', 'detached', 'impassive', 'resigned'],\n",
    "            'ambitious': ['ambitious', 'aspiring', 'driven', 'determined', 'goal-oriented', 'enterprising', 'striving'],\n",
    "            'honest': ['honest', 'truthful', 'sincere', 'candid', 'forthright', 'genuine', 'open'],\n",
    "            'greedy': ['greedy', 'avaricious', 'covetous', 'materialistic', 'grasping', 'self-indulgent'],\n",
    "            'generous': ['generous', 'charitable', 'giving', 'selfless', 'open-handed', 'philanthropic', 'bountiful'],\n",
    "            'jealous': ['jealous', 'envious', 'covetous', 'possessive', 'suspicious', 'resentful'],\n",
    "            'dutiful': ['dutiful', 'obedient', 'responsible', 'respectful', 'conscientious', 'loyal', 'reliable'],\n",
    "            'apathetic': ['apathetic', 'indifferent', 'unemotional', 'detached', 'unconcerned', 'uninvolved', 'listless']\n",
    "        }\n",
    "    \n",
    "    \n",
    "    #-------------------------------Step 1 : Getting the Text for the Books-----------------------------------\n",
    "    '''\n",
    "    With the first two methods I define the methods for fetching for the books I'll use. Ideally, this tool can be generalized to many other books. \n",
    "    These are fetched from Project Gutenberg (PG), a library of over 75,000 free eBooks of great classics. \n",
    "    Link to their main page [here](https://www.gutenberg.org/). \n",
    "    You are encouraged to try other books if any character is not known well enough to evaluate his/her personality. \n",
    "    Removing PG's metadata is also necessary to prevent errors and imprecisions.\n",
    "    '''   \n",
    "    def fetch_book(self, url: str, book_name: str):\n",
    "        \"\"\"\n",
    "        url is PG's URL for the book\n",
    "        \"\"\"\n",
    "        print(f\"fetching {book_name}...\")\n",
    "        response = requests.get(url)\n",
    "        # remove Project Gutenberg headers/footers calling the other method)\n",
    "        text = self._clean_gutenberg_text(response.text)\n",
    "        self.books[book_name] = text\n",
    "        print(f\"{book_name} loaded ({len(text)} characters)\")\n",
    "    \n",
    "    def _clean_gutenberg_text(self, text: str) -> str:\n",
    "        \"\"\" Remove PG metadata and clean text\n",
    "         Remove everything before \"START OF THE PROJECT GUTENBERG EBOOK\".\n",
    "         Visual inspection of a sample of ebooks shown that they all have this line, sometimes this won't exclude the \"front page\" or the Preface though, but this is a very minor problem      .\n",
    "         However in case a book would differ it should be easy to tweak the following line on a case by case scenario\n",
    "         \"\"\"\n",
    "        start_pattern = r'\\*\\*\\* START OF (?:THE |THIS )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*'\n",
    "        end_pattern = r'\\*\\*\\* END OF (?:THE |THIS )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*'\n",
    "        \n",
    "        start_match = re.search(start_pattern, text) #using regex to find the start/end\n",
    "        if start_match:\n",
    "            text = text[start_match.end():]\n",
    "        \n",
    "        end_match = re.search(end_pattern, text)\n",
    "        if end_match:\n",
    "            text = text[:end_match.start()]\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # ------------------------------------Step 2: Named Entity Recognition-----------------------------------------------\n",
    "    def extract_characters(self, book_name: str, target_characters: List[str] = None):\n",
    "        \"\"\"\n",
    "        Extract character mentions from the book using NER and/or targeted search.\n",
    "            \n",
    "        This is the core of the character detection. Using spacy's NER to find\n",
    "        PERSON entities, but it can also look for specific characters if provided.\n",
    "\n",
    "        target_characters is the optional list of specific characters to look for. If false, extracts all PERSON entities found by NER.\n",
    "        \"\"\"\n",
    "        # Check\n",
    "        if book_name not in self.books:\n",
    "            raise ValueError(f\"Book '{book_name}' not loaded\")\n",
    "        \n",
    "        if target_characters:\n",
    "            print(f\"Looking for specific characters in {book_name}: {target_characters}\")\n",
    "        else:\n",
    "            print(f\"Extracting all characters from {book_name}...\")\n",
    "            \n",
    "        text = self.books[book_name]\n",
    "        # process the text with spacy (yeah this does all the nlp stuff, tokenization, deoendency parsing, whitespace normalization etc)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        character_mentions = []\n",
    "        \n",
    "        # Acts differently depending on if the character list is present, either target search or ner \n",
    "        if target_characters:\n",
    "            # Look for specific characters through a later defined f\n",
    "            character_mentions = self._find_target_characters(doc, target_characters)\n",
    "        else:\n",
    "            # Extract all person entities through a later defined f\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    mention = self._create_character_mention(ent, doc)\n",
    "                    character_mentions.append(mention)\n",
    "        \n",
    "        self.character_mentions[book_name] = character_mentions\n",
    "        print(f\"Found {len(character_mentions)} character mentions\")\n",
    "    \n",
    "    def _find_target_characters(self, doc, target_characters: List[str]) -> List[CharacterMention]:\n",
    "        \"\"\"\n",
    "        Just picking the person token after ner. \n",
    "        Originally I tried to implement also other methods to enforce the results compared to just picking from ner but it was useless\n",
    "        \"\"\"\n",
    "        mentions = []\n",
    "        \n",
    "        # Base approach of trying to find them through ner\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                if self._is_target_character(ent.text, target_characters):\n",
    "                    mention = self._create_character_mention(ent, doc)\n",
    "                    mentions.append(mention)\n",
    "        #other approaches were removed as never beneficial\n",
    "        return mentions\n",
    "    \n",
    "    def _is_target_character(self, found_name: str, target_characters: List[str]) -> bool:\n",
    "        \"\"\"Check if a found name matches any target character\n",
    "        Input:\n",
    "            A single found name string\n",
    "            A list of target names\n",
    "        Output:\n",
    "            Bool (Belongs or does not)\n",
    "        \"\"\"\n",
    "        found_lower = found_name.lower()\n",
    "        for target in target_characters:\n",
    "            target_lower = target.lower()\n",
    "            # Check for exact match or partial match. I tried different implementation but no matter what it seems this is sometime not working\n",
    "            # this try to adress all the cases:\n",
    "            # case 1: target is in found name\n",
    "            # case 2: found name is in target\n",
    "            # case 3: any word from target appears in found name\n",
    "            if (target_lower in found_lower or \n",
    "                found_lower in target_lower or\n",
    "                any(part in found_lower for part in target_lower.split())):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _find_character_by_name(self, doc, character_name: str) -> List[CharacterMention]:\n",
    "        \"\"\"\n",
    "        Finds all occurrences of a specific name in the text\n",
    "        Input:\n",
    "            Spacy document\n",
    "            Single character name to search for\n",
    "        Output:\n",
    "            List of CharacterMention\n",
    "        \"\"\"\n",
    "        mentions = []\n",
    "        char_lower = character_name.lower()\n",
    "        char_words = char_lower.split()\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_lower = sent.text.lower()\n",
    "            \n",
    "            # Look for exact matches first\n",
    "            if char_lower in sent_lower:\n",
    "                start_idx = sent_lower.find(char_lower)\n",
    "                if start_idx != -1:\n",
    "                    mention = CharacterMention( #this part does basically the same both for exact and partial\n",
    "                        name=character_name,\n",
    "                        start=sent.start_char + start_idx,\n",
    "                        end=sent.start_char + start_idx + len(character_name),\n",
    "                        sentence=sent.text.strip(),\n",
    "                        doc_context=self._get_context_around_sentence(doc, sent) #also get context around the sentence\n",
    "                    )\n",
    "                    mentions.append(mention)\n",
    "            \n",
    "            # Again also look for partial matches \n",
    "            #Here I'm trying to consider all the cases like reverse\n",
    "            elif all(word in sent_lower for word in char_words):\n",
    "                tokens = [token.text.lower() for token in sent]\n",
    "                for i in range(len(tokens)):\n",
    "                    if tokens[i] == char_words[0]:\n",
    "                        if tokens[i:i+len(char_words)] == char_words:\n",
    "                            mention = CharacterMention( #this part does basically the same both for exact and partial\n",
    "                                name=character_name,\n",
    "                                start=sent[i].idx,\n",
    "                                end=sent[i+len(char_words)-1].idx + len(sent[i+len(char_words)-1].text),\n",
    "                                sentence=sent.text.strip(),\n",
    "                                doc_context=self._get_context_around_sentence(doc, sent) #also get context around the sentence\n",
    "                            )\n",
    "                            mentions.append(mention)\n",
    "        \n",
    "        return mentions\n",
    "    \n",
    "    def _create_character_mention(self, ent, doc) -> CharacterMention:\n",
    "        \"\"\"Create a Charactermention object from a spacy entity\"\"\"\n",
    "        sentence = ent.sent.text.strip()\n",
    "        context = self._get_context_around_sentence(doc, ent.sent)\n",
    "        \n",
    "        return CharacterMention(\n",
    "            name=ent.text,\n",
    "            start=ent.start_char,\n",
    "            end=ent.end_char,\n",
    "            sentence=sentence,\n",
    "            doc_context=context\n",
    "        )\n",
    "    \n",
    "    def _get_context_around_sentence(self, doc, sentence):\n",
    "        \"\"\"Get context around a sentence\"\"\"\n",
    "        sent_start = sentence.start\n",
    "        sent_end = sentence.end\n",
    "        context_start = max(0, sent_start - 50)  # 75 tokens before\n",
    "        context_end = min(len(doc), sent_end + 50)  # 75 tokens after\n",
    "        return doc[context_start:context_end].text\n",
    "    \n",
    "    #----------------------------------------Step 3: Character Name Variant Grouping----------------------------------------\n",
    "    \n",
    "    def group_character_variants(self, book_name: str, min_mentions: int = 3):\n",
    "        \"\"\"\n",
    "        Group character name variants (e.g., 'Victor', 'Victor Frankenstein', 'Dr. Frankenstein')\n",
    "        Takes a book name and minimum mention threshold (def 3)\n",
    "        \"\"\"\n",
    "        #Check\n",
    "        if book_name not in self.character_mentions:\n",
    "            raise ValueError(f\"no character mentions found for '{book_name}'\")\n",
    "        \n",
    "        print(f\"Grouping characters variants for {book_name}...\")\n",
    "        mentions = self.character_mentions[book_name] #get all the mentions\n",
    "        name_counts = Counter(mention.name for mention in mentions) # count all character names\n",
    "        \n",
    "        # check count of name vs threshold\n",
    "        frequent_names = {name for name, count in name_counts.items() if count >= min_mentions}\n",
    "        \n",
    "        # My goal here is to create another mapping from partials to full name, so {Frankenstein, Victor, V.Frankenstein} -> Victor Frankenstein.\n",
    "        \n",
    "        grouped_chars = {}  # create dict that will store the grouped character names\n",
    "        processed = set()   # keep track of already grouped names\n",
    "        #sorting longer names first to try to pick more complete names as canonical\n",
    "        for name in sorted(frequent_names, key=len, reverse=True):\n",
    "            if name in processed:\n",
    "                continue\n",
    "\n",
    "            canonical = name\n",
    "            variants = set([name])\n",
    "            name_parts = set(name.lower().split())\n",
    "\n",
    "            # look for any other names that share at least one part\n",
    "            for other_name in frequent_names:\n",
    "                if other_name == name or other_name in processed:\n",
    "                    continue\n",
    "                other_parts = set(other_name.lower().split())\n",
    "                if name_parts & other_parts:  # if they share any part\n",
    "                    variants.add(other_name)\n",
    "                    processed.add(other_name)\n",
    "\n",
    "            \n",
    "            processed.add(name)\n",
    "            grouped_chars[canonical] = variants\n",
    "\n",
    "        #store \n",
    "        for canonical, variants in grouped_chars.items():\n",
    "            self.character_variants[f\"{book_name}:{canonical}\"] = variants\n",
    "\n",
    "        print(f\"Grouped into {len(grouped_chars)} main characters:\")\n",
    "        for canonical, variants in sorted(grouped_chars.items()):\n",
    "            print(f\"  - {canonical}: {variants}\")\n",
    "\n",
    "        return grouped_chars\n",
    "\n",
    "    #---------------------------------------------- Step 4: Dependency Parsing and Adjective Extraction-----------------------------------\n",
    "    def extract_character_adjectives(self, book_name: str):\n",
    "        \"\"\"\n",
    "        Extracts the adjectives associated with character mentions using spacy dependency parsing, analyzing synctactic relationships between words.\n",
    "        Output:\n",
    "             Dictionary mapping character names to their associated adjectives\n",
    "        \"\"\"\n",
    "        print(f\"Extcracting adjectives for characters in {book_name}...\")\n",
    "        \n",
    "        character_adjectives = defaultdict(list)\n",
    "        mentions = self.character_mentions[book_name]\n",
    "        \n",
    "        for mention in mentions:\n",
    "            # process the context around the character mention, probably not the most efficient solution to re-run NLP, could there be a way to store pre-processed Doc\n",
    "            doc = nlp(mention.doc_context)\n",
    "            \n",
    "            # finding the character entity in the processed context\n",
    "            char_tokens = []\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"PERSON\" and self._names_match(ent.text, mention.name):\n",
    "                    char_tokens.extend(ent)\n",
    "            \n",
    "            #if we didnt find the entity, try to find tokens by text matching\n",
    "            if not char_tokens:\n",
    "                for token in doc:\n",
    "                    if mention.name.lower() in token.text.lower():\n",
    "                        char_tokens.append(token)\n",
    "            \n",
    "            # Extract adjectives related to these character tokens\n",
    "            for char_token in char_tokens:\n",
    "                adjectives = self._extract_adjectives_for_token(char_token)\n",
    "                character_adjectives[mention.name].extend(adjectives)\n",
    "        \n",
    "        return character_adjectives\n",
    "    \n",
    "    def _names_match(self, name1: str, name2: str) -> bool:\n",
    "        \"\"\"\n",
    "        Case-insensitive check if names share either:\n",
    "        full containment or any common word part.\n",
    "        \"\"\"\n",
    "        return (name1.lower() in name2.lower() or \n",
    "                name2.lower() in name1.lower() or\n",
    "                set(name1.lower().split()) & set(name2.lower().split()))\n",
    "    \n",
    "    def _extract_adjectives_for_token(self, token) -> List[str]:\n",
    "        \"\"\"\n",
    "            Extracts adjectives that are semantically or syntactically related to a given token.\n",
    "    \n",
    "            The function checks for a list of ways adjectives are typically used in written language, \n",
    "            further expanding the cases would improve the performance as my information on the subject is rather limited \n",
    "            Possibly there are cases beyond these 6.\n",
    "            1. Direct adj. (e.g. \"the ferocious Creature\")\n",
    "            2. Subject complements (\"the Creature is ferocious\")\n",
    "            3. Clause descriptions (\"the Creature, who was ferocious, attacked\")\n",
    "            4. Appositional adj. (\"the Creature, ferocious, attacked.)\n",
    "            5. Conjoined adj. (\"brutal and cunning\")\n",
    "            6. Adj. in the same noun phrase (\"the brutal and cunning Creature\")\n",
    "        \"\"\"\n",
    "        adjectives = []\n",
    "        \n",
    "        # 1. Direct modifiers \n",
    "        for child in token.children:\n",
    "            if child.dep_ == \"amod\":\n",
    "                adjectives.append(child.lemma_.lower())\n",
    "        \n",
    "        # 2. Subject complements\n",
    "        if token.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in [\"acomp\", \"attr\"] and child.pos_ == \"ADJ\":\n",
    "                    adjectives.append(child.lemma_.lower())\n",
    "        \n",
    "        # 3. Clause descriptions\n",
    "        for ancestor in token.ancestors:\n",
    "            for child in ancestor.children:\n",
    "                if child.dep_ == \"relcl\" and child.pos_ == \"VERB\":\n",
    "                    for adj in child.subtree:\n",
    "                        if adj.pos_ == \"ADJ\":\n",
    "                            adjectives.append(adj.lemma_.lower())\n",
    "        \n",
    "        # 4. Appositional adj.\n",
    "        if token.dep_ == \"appos\":\n",
    "            for child in token.head.children:\n",
    "                if child.pos_ == \"ADJ\":\n",
    "                    adjectives.append(child.lemma_.lower())\n",
    "        \n",
    "        # 5. Conjoined adj.\n",
    "        if token.dep_ == \"conj\":\n",
    "            if token.head.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.head.lemma_.lower())\n",
    "        \n",
    "        # 6. Adj. in the same phrase\n",
    "        for ancestor in token.ancestors:\n",
    "            if ancestor.pos_ == \"NOUN\":\n",
    "                for child in ancestor.children:\n",
    "                    if child.pos_ == \"ADJ\" and child != token:\n",
    "                        adjectives.append(child.lemma_.lower())\n",
    "        \n",
    "        return list(set(adjectives))  # Remove duplicates\n",
    "    \n",
    "    #--------------------------------------------- Step 5: Personality Trait Mapping----------------------------------\n",
    "    def map_adjectives_to_traits(self, character_adjectives: Dict[str, List[str]]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Map adjectives to personality traits and calculate scores\n",
    "        Input:\n",
    "            dict of character to adj.\n",
    "        OUtput:\n",
    "            dict of character to dict of traits to score\n",
    "        \"\"\"\n",
    "        character_personalities = {}\n",
    "        \n",
    "        for character, adjectives in character_adjectives.items():\n",
    "            trait_scores = defaultdict(float)\n",
    "            adj_counter = Counter(adjectives)\n",
    "            \n",
    "            # calculate score for each trait\n",
    "            for trait, trait_adjectives in self.personality_traits.items():\n",
    "                score = 0\n",
    "                for adj in trait_adjectives:\n",
    "                    if adj in adj_counter:\n",
    "                        score += adj_counter[adj]\n",
    "                \n",
    "               \n",
    "                total_adj = sum(adj_counter.values())\n",
    "                if 0 < total_adj:\n",
    "                    trait_scores[trait] = score\n",
    "                    \n",
    "            character_personalities[character] = dict(trait_scores)\n",
    "        \n",
    "        return character_personalities\n",
    "    \n",
    "    # -----------------------------------------Step 6: Complete Analysis pipeline---------------------------------------------\n",
    "    def analyze_book(self, url: str, book_name: str, target_characters: List[str] = None):\n",
    "        \"\"\"\n",
    "        This is the final function that executes the 5 steps of the character personality analysis pipeline, \n",
    "        from URL (and possibly character list) to final trait profiles.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Analyzing {book_name} ===\")\n",
    "        \n",
    "        # Step 1: Fetch book\n",
    "        self.fetch_book(url, book_name)\n",
    "        \n",
    "        # Step 2: Extract characters\n",
    "        self.extract_characters(book_name, target_characters)\n",
    "    \n",
    "        # Step 3: Group character variants\n",
    "        character_groups = self.group_character_variants(book_name)\n",
    "        \n",
    "        # Step 4: Extract adjectives\n",
    "        character_adjectives = self.extract_character_adjectives(book_name)\n",
    "        \n",
    "        # Merge adjectives for character variants\n",
    "        merged_adjectives = defaultdict(list)\n",
    "        for canonical, variants in character_groups.items():\n",
    "            for variant in variants:\n",
    "                if variant in character_adjectives:\n",
    "                    merged_adjectives[canonical].extend(character_adjectives[variant])\n",
    "    \n",
    "        # Step 5: Map to personality traits\n",
    "        personalities = self.map_adjectives_to_traits(merged_adjectives)\n",
    "        #Printing\n",
    "        print(f\"\\n==== Character personalities in {book_name} ===\")\n",
    "        has_results = False\n",
    "        for character, traits in personalities.items():\n",
    "            if any(score > 0 for score in traits.values()):\n",
    "                has_results = True\n",
    "                print(f\"\\n{character}:\")\n",
    "                sorted_traits = sorted(traits.items(), key=lambda x: x[1], reverse=True)\n",
    "                for trait, score in sorted_traits[:5]:  # Top 5 traits sorted by importance (score)\n",
    "                    if score > 0:\n",
    "                        label = self._get_trait_label(score)\n",
    "                        print(f\"  {trait}: ({label})\")\n",
    "        \n",
    "        if not has_results:\n",
    "            print(\"\\nNo personality trait found for any characters\")\n",
    "        \n",
    "        return personalities\n",
    "\n",
    "    \n",
    "    def _get_trait_label(self, score: float) -> str:\n",
    "        \"\"\"Get descriptive label for a trait score\"\"\"\n",
    "        if score == 1:\n",
    "            return \"has been called\"\n",
    "        elif 2 <= score <= 3:\n",
    "            return \"is often\"\n",
    "        elif 4 <= score <= 5:\n",
    "            return \"is very\"\n",
    "        elif 6 <= score <= 7:\n",
    "            return \"is incredibly\"\n",
    "        elif score >= 8:\n",
    "            return \"is a paragon of\"\n",
    "#end of the very long class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06759de1",
   "metadata": {},
   "source": [
    "Here's a collection of other url for books you may want to try (I didn't try them all, these are just for easy of use):\n",
    "\n",
    "Pride and Prejudice (Jane Austen) https://www.gutenberg.org/files/1342/1342-0.txt\n",
    "\n",
    "Moby Dick (Herman Melville) https://www.gutenberg.org/files/2701/2701-0.txt\n",
    "\n",
    "Dracula (Bram Stoker) https://www.gutenberg.org/files/345/345-0.txt\n",
    "\n",
    "Crime and Punishment (Dostoevsky) https://www.gutenberg.org/files/2554/2554-0.txt\n",
    "\n",
    "The Strange Case of Dr. Jekyll and Mr. Hyde https://www.gutenberg.org/files/43/43-0.txt\n",
    "\n",
    "The Picture of Dorian Gray (Oscar Wilde) https://www.gutenberg.org/files/174/174-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3894f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analyzing Frankenstein ===\n",
      "fetching Frankenstein...\n",
      "Frankenstein loaded (426692 characters)\n",
      "Looking for specific characters in Frankenstein: ['Victor Frankenstein', 'Elizabeth', 'Creature', 'Clerval']\n",
      "Found 191 character mentions\n",
      "Grouping characters variants for Frankenstein...\n",
      "Grouped into 4 main characters:\n",
      "  - Clerval: {'Clerval'}\n",
      "  - Elizabeth Lavenza: {'Elizabeth Lavenza', 'Elizabeth'}\n",
      "  - Frankenstein: {'Frankenstein'}\n",
      "  - Victor: {'Victor'}\n",
      "Extcracting adjectives for characters in Frankenstein...\n",
      "\n",
      "==== Character personalities in Frankenstein ===\n",
      "\n",
      "Elizabeth Lavenza:\n",
      "  brave: (has been called)\n",
      "  heroic: (has been called)\n",
      "\n",
      "Frankenstein:\n",
      "  just: (has been called)\n",
      "\n",
      "Clerval:\n",
      "  heroic: (has been called)\n",
      "\n",
      "=== Analyzing The Betrothed ===\n",
      "fetching The Betrothed...\n",
      "The Betrothed loaded (966279 characters)\n",
      "Looking for specific characters in The Betrothed: ['Lorenzo Tramaglino', 'Lucia Mondella', 'Don Rodrigo', 'Fra Cristoforo', 'Don Abbondio']\n",
      "Found 947 character mentions\n",
      "Grouping characters variants for The Betrothed...\n",
      "Grouped into 4 main characters:\n",
      "  - Don Abbondio’s: {'Don Abbondio', 'Don Roderick', 'Don\\r\\nRoderick', 'Don\\r\\nAbbondio', 'Don Gonzalo', 'Don Abbondio’s', 'Don\\r\\nGonzalo', 'Don Ferrante'}\n",
      "  - Donna Prassede: {'Donna Prassede'}\n",
      "  - Lorenzo Tramaglino: {'Renzo Tramaglino', 'Tramaglino', 'Lorenzo Tramaglino'}\n",
      "  - Renzo: {'Renzo'}\n",
      "Extcracting adjectives for characters in The Betrothed...\n",
      "\n",
      "==== Character personalities in The Betrothed ===\n",
      "\n",
      "Don Abbondio’s:\n",
      "  jealous: (is very)\n",
      "  proud: (is often)\n",
      "  strong: (has been called)\n",
      "  complex: (has been called)\n",
      "\n",
      "Renzo:\n",
      "  kind: (has been called)\n",
      "  malicious: (has been called)\n",
      "  just: (has been called)\n",
      "  honest: (has been called)\n",
      "\n",
      "=== Analyzing Frankeinstein with NER ===\n",
      "\n",
      "=== Analyzing Frankenstein ===\n",
      "fetching Frankenstein...\n",
      "Frankenstein loaded (426692 characters)\n",
      "Extracting all characters from Frankenstein...\n",
      "Found 528 character mentions\n",
      "Grouping characters variants for Frankenstein...\n",
      "Grouped into 21 main characters:\n",
      "  - Adam: {'Adam'}\n",
      "  - Agatha: {'Agatha'}\n",
      "  - Albertus Magnus: {'Albertus Magnus'}\n",
      "  - Clerval: {'Clerval'}\n",
      "  - Cornelius Agrippa: {'Cornelius Agrippa'}\n",
      "  - De Lacey: {'De Lacey'}\n",
      "  - Elizabeth Lavenza: {'Elizabeth Lavenza', 'Elizabeth'}\n",
      "  - Ernest: {'Ernest'}\n",
      "  - Felix: {'Felix'}\n",
      "  - Frankenstein: {'Frankenstein'}\n",
      "  - Henry: {'Henry'}\n",
      "  - Justine Moritz: {'Justine', 'Justine Moritz'}\n",
      "  - Kirwin: {'Kirwin'}\n",
      "  - M. Waldman: {'M. Krempe', 'M. Waldman'}\n",
      "  - Margaret: {'Margaret'}\n",
      "  - Paracelsus: {'Paracelsus'}\n",
      "  - Saville: {'Saville'}\n",
      "  - Turk: {'Turk'}\n",
      "  - Victor: {'Victor'}\n",
      "  - Walton: {'Walton'}\n",
      "  - William: {'William'}\n",
      "Extcracting adjectives for characters in Frankenstein...\n",
      "\n",
      "==== Character personalities in Frankenstein ===\n",
      "\n",
      "Elizabeth Lavenza:\n",
      "  brave: (has been called)\n",
      "  heroic: (has been called)\n",
      "\n",
      "Justine Moritz:\n",
      "  naive: (is incredibly)\n",
      "\n",
      "Frankenstein:\n",
      "  just: (has been called)\n",
      "\n",
      "M. Waldman:\n",
      "  kind: (is very)\n",
      "  humble: (has been called)\n",
      "\n",
      "Clerval:\n",
      "  heroic: (has been called)\n",
      "\n",
      "Agatha:\n",
      "  kind: (is often)\n",
      "\n",
      "Henry:\n",
      "  loyal: (has been called)\n",
      "  sensitive: (has been called)\n",
      "\n",
      "Felix:\n",
      "  weak: (is often)\n"
     ]
    }
   ],
   "source": [
    "# Usage Examples\n",
    "def main():\n",
    "    #----------------------------- Examples Analyzing with specific characters list--------------------------------------\n",
    "    analyzer = CharacterPersonalityAnalyzer()\n",
    "    \n",
    "    # Analyze Frankenstein with specific characters.\n",
    "    frankenstein_characters = [\n",
    "        'Victor Frankenstein', 'Elizabeth', 'Creature', 'Clerval'\n",
    "    ]\n",
    "    \n",
    "    frankenstein_personalities = analyzer.analyze_book(\n",
    "        \"https://www.gutenberg.org/files/84/84-0.txt\", \n",
    "        \"Frankenstein\",\n",
    "        target_characters=frankenstein_characters\n",
    "    )\n",
    "  \n",
    "    # Analyze The Betrothed with specific characters\n",
    "    betrothed_characters = [\n",
    "        'Lorenzo Tramaglino', \n",
    "        'Lucia Mondella',\n",
    "        'Don Rodrigo',\n",
    "        'Fra Cristoforo',\n",
    "        'Don Abbondio'\n",
    "    ]\n",
    "    \n",
    "    betrothed_personalities = analyzer.analyze_book(\n",
    "        \"https://www.gutenberg.org/files/35155/35155-0.txt\", \n",
    "        \"The Betrothed\",\n",
    "        target_characters=betrothed_characters\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    \n",
    "    #-------------------------------- Examples with pure NER (no target characters) (these are obviously longer, be patient)--------------------\n",
    "    '''\n",
    "     Discover relevant characters through NER\n",
    "     analyzer.analyze_book(url, book_name)   (No target_characters parameter)\n",
    "    '''\n",
    "    analyzer = CharacterPersonalityAnalyzer()\n",
    "    \n",
    "    #print(\"\\n=== Analyzing Pride and Prejudice with NER ===\")\n",
    "    #pride_prejudice_personalities = analyzer.analyze_book(\n",
    "        #\"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "        #\"Pride and Prejudice\"\n",
    "    #)\n",
    "    print(\"\\n=== Analyzing Frankeinstein with NER ===\")\n",
    "    Frankeinstein_personalities = analyzer.analyze_book(\n",
    "        \"https://www.gutenberg.org/files/84/84-0.txt\",\n",
    "        \"Frankenstein\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a53d6f",
   "metadata": {},
   "source": [
    "\n",
    "# INTERPRETING RESULTS\n",
    " The personality traits extracted for characters depend heavily on how often and how explicitly traits are attributed to them through adjectives. \n",
    " It is also to note that the decision to not normalize the results was taken, so while it is indeed true that if a character is called 3 times cool he's probably a cool guy, it is also true that longer novels have better chance of describing some characters in some way that is captured by this program.\n",
    " Running this code on \"The Sentinel\" of Arthur C. Clarke is therefore likely to yeld no result.\n",
    " Note that this alspo means characters that are central on average receive more descriptive attention, so their personality profiles tend to be more complete. Less central characters often fall below the mention threshold or lack adjectives captured in my mapping altogether.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f9120",
   "metadata": {},
   "source": [
    "# Limitations              \n",
    "1. The system struggles handling typical nicknames, like \"Renzo\" for \"Lorenzo\" or \"Lizzy\" for \"Elizabeth\" . This is also true for variation of the same entity, like \"The Creature\" or \"The Monster\" that are considered two different beings. This could be mitgated by also implementing a mapping of most common nicknames.\n",
    "\n",
    "2. Viktor Frankeinstein gave me a few problems: there are more than one F. in the book, but due to the style of the book he is rarely adressed by name, even less often to attribute him adjectives.\n",
    "\n",
    "3. Depending on writing style characters are described by actions and not adjectives. Frankenstein's obsession for death is never described explicity as \"V.F. is obsessed by the idea of defeating death\". This key component of his personality is instead easily understandable by his vicissitudes. A good number of authors of more famous books use rarely direct descriptions of their characters.\n",
    "\n",
    "# Possible improvements \n",
    "1. Pronouns (\"he,\" \"the scientist\" etc.) divide trait attribution by treating references to the same character as separate entities. From my understanding, Coreference resolution tools could stitch these fragments into unified profiles, but I failed to implement it in the code as in Spacy it is relegated to the experimental package, which I couldn't add to the pipeline as it was exclusive of an older version of spacy. \n",
    "\n",
    "2. There are a few things (e.g. the trait-adjective map, the adjective extractor function) that the larger and more accurate they are the better this code will perform, expanding those more carefully would improve the results.\n",
    "\n",
    "3. Manual mappings for tasks like adjective extraction limit scalability. I had to hardcode several part of this code due to my inability to found already made python libraries, but finding and using something like that, provided they are properly formulated, would improve the results. Another road could be to rely more on Neural Networks (Bert-based systems specifically) for these tasks they excel in generalizing, but I left them out on purpose to avoid solving all the problems with the same tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tasd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
